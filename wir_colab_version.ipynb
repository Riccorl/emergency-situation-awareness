{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wir_colab_version.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riccorl/emergency-situation-awareness/blob/master/wir_colab_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzV4OL1vM90O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://www.dropbox.com/s/owl26mc9n0ruxop/data.zip\n",
        "!unzip data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPw6Qmd9QXFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://crisisnlp.qcri.org/data/lrec2016/crisisNLP_word2vec_model_v1.2.zip\n",
        "!unzip crisisNLP_word2vec_model_v1.2.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A141TxaKRvRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r __MACOSX\n",
        "!mkdir  output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg3BtfO9PkV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsoRGjP7TyA1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a4d3d9e2-b45e-4b9d-be8d-4737cf459e92"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxgmsTNYJji9",
        "colab_type": "text"
      },
      "source": [
        "app.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6XqsZ56Jgu0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "outputId": "2239fa70-1333-4419-84a4-c48f3a3d1449"
      },
      "source": [
        "import gensim\n",
        "\n",
        "# import config\n",
        "# import evaluation\n",
        "# import preprocess\n",
        "# import train\n",
        "# import utils\n",
        "\n",
        "\n",
        "def process():\n",
        "    print(\"Loading tweets...\")\n",
        "    features, labels = load_datasets(\n",
        "        CRISIS_TRAIN_DIR, NORMAL_TRAIN_DIR\n",
        "    )\n",
        "    print(\"Clearing tweets...\")\n",
        "    features = clear_tweets(features)\n",
        "    print(features[0])\n",
        "    print(\"Loading pre-trained embeddings...\")\n",
        "    # load the w2v matrix with genism\n",
        "    w2v = gensim.models.KeyedVectors.load_word2vec_format(\n",
        "        CRISIS_PRE_TRAINED, binary=True\n",
        "    )\n",
        "    # build the vocab from the w2v model\n",
        "    w2v_vocab = vocab_from_w2v(w2v)\n",
        "    print(\"Word2Vec model vocab len:\", len(w2v_vocab))\n",
        "    # build vocab from the dataset\n",
        "    data_vocab = build_vocab([features])\n",
        "    # filter pretrained w2v with words from the dataset\n",
        "    w2v = restrict_w2v(w2v, set(data_vocab.keys()))\n",
        "    w2v_vocab = vocab_from_w2v(w2v)\n",
        "    write_dictionary(TRAIN_VOCAB, w2v_vocab)\n",
        "    print(\"Cleaned vocab len:\", len(w2v_vocab))\n",
        "    # idx2word = {v: k for k, v in vocab.items()}\n",
        "    return features, labels, data_vocab, w2v, w2v_vocab\n",
        "\n",
        "\n",
        "def main():\n",
        "    features, labels, vocab, w2v, w2v_vocab = process()\n",
        "    model = train_keras(features, labels, w2v=w2v, w2v_vocab=w2v_vocab, batch_size=2048)\n",
        "    evaluation.evaluate(model)\n",
        "\n",
        "\n",
        "main()\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading tweets...\n",
            "Number of crisis tweets: 270000\n",
            "Number of non-crisis tweets: 420000\n",
            "Clearing tweets...\n",
            "['two', '#saudis', 'die', '#mers', 'virus', 'toll', 'nears', '100']\n",
            "Loading pre-trained embeddings...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Word2Vec model vocab len: 2152856\n",
            "Cleaned vocab len: 100640\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:72: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, None, 300)         30192000  \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 200)               240600    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 50)                10050     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 30,442,701\n",
            "Trainable params: 250,701\n",
            "Non-trainable params: 30,192,000\n",
            "_________________________________________________________________\n",
            "Starting training...\n",
            "Epoch 1/5\n",
            " 23/151 [===>..........................] - ETA: 3:22 - loss: 0.4158 - acc: 0.8015"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-094eb334cd2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-094eb334cd2d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_keras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw2v_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-10fcf58d0e9f>\u001b[0m in \u001b[0;36mtrain_keras\u001b[0;34m(features, labels, w2v, w2v_vocab, epochs, hidden_size, batch_size)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_dev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;31m# callbacks=[es, cp],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7PTI4OAJptD",
        "colab_type": "text"
      },
      "source": [
        "config.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tatUhSU6Jrju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pathlib\n",
        "\n",
        "# directories\n",
        "DATA_DIR = pathlib.Path(\"data\") # .resolve().parent.parent / \"data\"\n",
        "RES_DIR = pathlib.Path(\"resources\") # .resolve().parent.parent / \"resources\"\n",
        "OUTPUT_DIR = pathlib.Path(\"output\") # RES_DIR / \"output\"\n",
        "VOCABS_DIR = RES_DIR / \"vocabs\"\n",
        "\n",
        "TRAIN_DIR = pathlib.Path(\"train\")\n",
        "CRISIS_TRAIN_DIR = TRAIN_DIR / \"crisisnlp\"\n",
        "CRISISLEX_DIR = TRAIN_DIR / \"crisislex\"\n",
        "NORMAL_TRAIN_DIR = TRAIN_DIR / \"normal\"\n",
        "EVAL_DIR = pathlib.Path(\"evaluation\") # DATA_DIR / \"evaluation\"\n",
        "CRISIS_EVAL_DIR = EVAL_DIR / \"crisisnlp\"\n",
        "NORMAL_EVAL_DIR = EVAL_DIR / \"normal\"\n",
        "\n",
        "# embeddings\n",
        "CRISIS_PRE_TRAINED = pathlib.Path(\"crisisNLP_word2vec_model\") / \"crisisNLP_word_vector.bin\" # RES_DIR / \"embeddings\" / \"crisisNLP_emb.bin\"\n",
        "\n",
        "# vocabs\n",
        "TRAIN_VOCAB = pathlib.Path(\"train_vocab.txt\") # VOCABS_DIR / \"train_vocab.txt\"\n",
        "\n",
        "# crisis nlp\n",
        "PAKISTAN_EQ_TWEETS = CRISIS_TRAIN_DIR / \"2013_pakistan_eq.csv\"\n",
        "CALIFORNIA_EQ_TWEETS = CRISIS_TRAIN_DIR / \"2014_california_eq.csv\"\n",
        "CHILE_EQ_TWEETS = CRISIS_TRAIN_DIR / \"2014_chile_eq.csv\"\n",
        "NEPAL_EQ_TWEETS = CRISIS_TRAIN_DIR / \"2015_nepal_eq_cf_labels.csv\"\n",
        "EBOLA_TWEETS = CRISIS_TRAIN_DIR / \"2014_ebola_virus.csv\"\n",
        "ODILE_HR_TWEETS = CRISIS_TRAIN_DIR / \"2014_hurricane_odile.csv\"\n",
        "HAGUPIT_HR_TWEETS = CRISIS_TRAIN_DIR / \"2014_typhoon_hagupit_cf_labels.csv\"\n",
        "PAM_HR_TWEETS = CRISIS_TRAIN_DIR / \"2015_cyclone_pam_cf_labels.csv\"\n",
        "MERS_TWEETS = CRISIS_TRAIN_DIR / \"2014_mers_cf_labels.csv\"\n",
        "PAKISTAN_FL_TWEETS = CRISIS_TRAIN_DIR / \"2014_pakistan_floods_cf_labels.csv\"\n",
        "INDIA_FL_TWEETS = CRISIS_TRAIN_DIR / \"2014_india_floods.csv\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUzf_Sz_JthB",
        "colab_type": "text"
      },
      "source": [
        "models.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PYAYl8dJwUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as tf_hub\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras import Model\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.keras.layers import (\n",
        "    Dense,\n",
        "    Embedding,\n",
        "    Bidirectional,\n",
        "    LSTM,\n",
        "    Input,\n",
        "    Layer,\n",
        ")\n",
        "\n",
        "\n",
        "def build_model(\n",
        "    layer: keras.layers = LSTM,\n",
        "    hidden_size: int = 256,\n",
        "    input_length: int = None,\n",
        "    dropout: float = 0.2,\n",
        "    recurrent_dropout: float = 0.1,\n",
        "    learning_rate: float = 0.002,\n",
        "    vocab_size: int = None,\n",
        "    word2vec: gensim.models.word2vec.Word2Vec = None,\n",
        "    embedding_size: int = 300,\n",
        "    train_embeddings: bool = False,\n",
        ") -> Model:\n",
        "\n",
        "    input_layer = Input(shape=(input_length,))\n",
        "    if word2vec:\n",
        "        em = get_keras_embedding(word2vec, train_embeddings)(input_layer)\n",
        "    else:\n",
        "        em = Embedding(\n",
        "            vocab_size, embedding_size, input_length=input_length, mask_zero=True\n",
        "        )(input_layer)\n",
        "    # em = ELMoEmbedding(idx2word=idx2word, output_mode=\"default\", trainable=True)(input_layer)\n",
        "\n",
        "    lstm1 = Bidirectional(\n",
        "        layer(\n",
        "            units=hidden_size,\n",
        "            dropout=dropout,\n",
        "            recurrent_dropout=recurrent_dropout,\n",
        "            return_sequences=False,\n",
        "        )\n",
        "    )(em)\n",
        "    # lstm2 = Bidirectional(\n",
        "    #     layer(units=hidden_size, dropout=dropout, recurrent_dropout=recurrent_dropout)\n",
        "    # )(lstm1)\n",
        "\n",
        "    dense = Dense(50, activation=\"relu\")(lstm1)\n",
        "    output = Dense(1, activation=\"sigmoid\")(dense)\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "    optimizer = keras.optimizers.Adam()\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"acc\"])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_keras_embedding(\n",
        "    word2vec: gensim.models.word2vec.Word2Vec, trainable: bool = False\n",
        ") -> Embedding:\n",
        "    \"\"\"\n",
        "    Return a Tensorflow Keras 'Embedding' layer with weights set as the\n",
        "    Word2Vec model's learned word embeddings.\n",
        "    :param word2vec: gensim Word2Vec model\n",
        "    :param trainable: if False, the weights are frozen and stopped from being updated.\n",
        "                      If True, the weights can/will be further trained/updated.\n",
        "    :return: a tf.keras.layers.Embedding layer.\n",
        "    \"\"\"\n",
        "    weights = word2vec.wv.vectors\n",
        "    # random vector for pad\n",
        "    pad = np.random.rand(1, weights.shape[1])\n",
        "    # mean vector for unknowns\n",
        "    unk = np.mean(weights, axis=0, keepdims=True)\n",
        "    weights = np.concatenate((pad, unk, weights))\n",
        "\n",
        "    return Embedding(\n",
        "        input_dim=weights.shape[0],\n",
        "        output_dim=weights.shape[1],\n",
        "        weights=[weights],\n",
        "        mask_zero=True,\n",
        "        trainable=trainable,\n",
        "    )\n",
        "\n",
        "\n",
        "class ELMoEmbedding(Layer):\n",
        "    def __init__(self, idx2word, output_mode=\"default\", trainable=True, **kwargs):\n",
        "        assert output_mode in [\n",
        "            \"default\",\n",
        "            \"word_emb\",\n",
        "            \"lstm_outputs1\",\n",
        "            \"lstm_outputs2\",\n",
        "            \"elmo\",\n",
        "        ]\n",
        "        assert trainable in [True, False]\n",
        "        self.idx2word = idx2word\n",
        "        self.output_mode = output_mode\n",
        "        self.trainable = trainable\n",
        "        self.max_length = None\n",
        "        self.word_mapping = None\n",
        "        self.lookup_table = None\n",
        "        self.elmo_model = None\n",
        "        self.embedding = None\n",
        "        super(ELMoEmbedding, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.max_length = input_shape[1]\n",
        "        self.word_mapping = [\n",
        "            x[1] for x in sorted(self.idx2word.items(), key=lambda x: x[0])\n",
        "        ]\n",
        "        self.lookup_table = tf.contrib.lookup.index_to_string_table_from_tensor(\n",
        "            self.word_mapping, default_value=\"<UNK>\"\n",
        "        )\n",
        "        self.lookup_table.init.run(session=K.get_session())\n",
        "        self.elmo_model = tf_hub.Module(\n",
        "            \"https://tfhub.dev/google/elmo/2\", trainable=self.trainable\n",
        "        )\n",
        "        super(ELMoEmbedding, self).build(input_shape)\n",
        "\n",
        "    def call(self, x, **kwargs):\n",
        "        x = tf.cast(x, dtype=tf.int64)\n",
        "        sequence_lengths = tf.cast(tf.count_nonzero(x, axis=1), dtype=tf.int32)\n",
        "        strings = self.lookup_table.lookup(x)\n",
        "        inputs = {\"tokens\": strings, \"sequence_len\": sequence_lengths}\n",
        "        return self.elmo_model(inputs, signature=\"tokens\", as_dict=True)[\n",
        "            self.output_mode\n",
        "        ]\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if self.output_mode == \"default\":\n",
        "            return input_shape[0], 1024\n",
        "        if self.output_mode == \"word_emb\":\n",
        "            return input_shape[0], self.max_length, 512\n",
        "        if self.output_mode == \"lstm_outputs1\":\n",
        "            return input_shape[0], self.max_length, 1024\n",
        "        if self.output_mode == \"lstm_outputs2\":\n",
        "            return input_shape[0], self.max_length, 1024\n",
        "        if self.output_mode == \"elmo\":\n",
        "            return input_shape[0], self.max_length, 1024\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\"idx2word\": self.idx2word, \"output_mode\": self.output_mode}\n",
        "        return list(items())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8mxPNIKJxsj",
        "colab_type": "text"
      },
      "source": [
        "sequence.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH3wSBxSJ0Xy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Dict\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.keras.utils import Sequence\n",
        "\n",
        "\n",
        "class TextSequence(Sequence):\n",
        "    def __init__(\n",
        "        self, x_set, y_set, batch_size, vocab, max_len, num_classes: int = None\n",
        "    ):\n",
        "        self.x, self.y = x_set, y_set\n",
        "        self.batch_size = batch_size\n",
        "        self.num_classes = num_classes\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.compute_x(\n",
        "            self.x[idx * self.batch_size : (idx + 1) * self.batch_size],\n",
        "            self.vocab,\n",
        "            max_len=self.max_len,\n",
        "        )\n",
        "        batch_y = np.array(self.y[idx * self.batch_size : (idx + 1) * self.batch_size])\n",
        "\n",
        "        return batch_x, batch_y\n",
        "\n",
        "    def compute_x(\n",
        "        self, features, vocab: Dict[str, int], max_len: int = 200, pad: bool = True\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute the features X.\n",
        "        :param features: feature file.\n",
        "        :param vocab: vocab.\n",
        "        :param max_len: max len to pad.\n",
        "        :param pad: If True pad the matrix, otherwise return the matrix not padded.\n",
        "        :return: the feature vectors.\n",
        "        \"\"\"\n",
        "        data = [\n",
        "            [vocab[word] if word in vocab else vocab[\"<UNK>\"] for word in l]\n",
        "            for l in features\n",
        "        ]\n",
        "        if pad:\n",
        "            return pad_sequences(\n",
        "                data, truncating=\"post\", padding=\"post\", maxlen=max_len\n",
        "            )\n",
        "        else:\n",
        "            return np.array(data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4U8NS6lJ1gX",
        "colab_type": "text"
      },
      "source": [
        "preprocess.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SujRfhoBJ5r1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import string\n",
        "from typing import Dict, List, Set\n",
        "\n",
        "import gensim\n",
        "import numpy as np\n",
        "from keras_preprocessing.text import maketrans\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "\n",
        "def build_vocab(data: List[List[str]]) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Compute the vocab from the bigrams\n",
        "    :param data: data set files\n",
        "    :return: Dictionary from bigram to int\n",
        "    \"\"\"\n",
        "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "    for dataset in data:\n",
        "        for sentence in dataset:\n",
        "            for word in sentence:\n",
        "                if word not in vocab:\n",
        "                    vocab[word] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def vocab_from_w2v(word2vec: gensim.models.word2vec.Word2Vec) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    :param word2vec: trained Gensim Word2Vec model\n",
        "    :return: a dictionary from token to int\n",
        "    \"\"\"\n",
        "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "    for index, word in enumerate(word2vec.wv.index2word):\n",
        "        vocab[word] = index + 2\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def clear_tweets(tweets: List[str]) -> List:\n",
        "    \"\"\"\n",
        "    Clear list of tweets.\n",
        "    :param tweets: list of tweets.\n",
        "    :return: cleared tweets.\n",
        "    \"\"\"\n",
        "    # preprocessing stuffs\n",
        "    stops = set(stopwords.words(\"english\")) | set(string.punctuation)\n",
        "    html_regex = re.compile(r\"^https?:\\/\\/.*[\\r\\n]*\")\n",
        "    tokenizer = TweetTokenizer()\n",
        "    return [_clear_tweet(tweet, tokenizer, stops, html_regex) for tweet in tweets]\n",
        "\n",
        "\n",
        "def _clear_tweet(tweet: str, tokenizer, stops: Set, html_regex) -> List:\n",
        "    \"\"\"\n",
        "    Clean the tweet in input.\n",
        "    :param tweet: tweet to clean.\n",
        "    :param stops: set of stop words and punctuation to remove.\n",
        "    :return: tweet cleaned.\n",
        "    \"\"\"\n",
        "    return [word for word in tokenizer.tokenize(tweet.lower()) if word not in stops and not html_regex.search(word)]\n",
        "\n",
        "\n",
        "def compute_x(\n",
        "    features, vocab: Dict[str, int], max_len: int = 200, pad: bool = True\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute the features X.\n",
        "    :param features: feature file.\n",
        "    :param vocab: vocab.\n",
        "    :param max_len: max len to pad.\n",
        "    :param pad: If True pad the matrix, otherwise return the matrix not padded.\n",
        "    :return: the feature vectors.\n",
        "    \"\"\"\n",
        "    data = [\n",
        "        [vocab[word] if word in vocab else vocab[\"<UNK>\"] for word in l]\n",
        "        for l in features\n",
        "    ]\n",
        "    if pad:\n",
        "        return pad_sequences(data, truncating=\"post\", padding=\"post\", maxlen=max_len)\n",
        "    else:\n",
        "        return np.array(data)\n",
        "\n",
        "\n",
        "def batch_generator(\n",
        "    features: List[str],\n",
        "    labels: List[str],\n",
        "    vocab: Dict[str, int],\n",
        "    batch_size: int = 32,\n",
        "    max_input_len: int = 0,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates batches of data from features and labels,\n",
        "    use it with Keras model.\n",
        "    :param features: list of unigrams feature\n",
        "    :param labels: list of labels\n",
        "    :param vocab: unigram vocab\n",
        "    :param batch_size: size of the batch to yield\n",
        "    :param n_classes: number of classes\n",
        "    :param max_input_len: max len of the input\n",
        "    :return: processed features and labels, in batches\n",
        "    \"\"\"\n",
        "\n",
        "    while True:\n",
        "        for start in range(0, len(features), batch_size):\n",
        "            end = start + batch_size\n",
        "            max_len = len(max(features[start:end], key=len))\n",
        "\n",
        "            if max_input_len > 0:\n",
        "                # truncate the sequence\n",
        "                max_len = max_len if max_len < max_input_len else max_input_len\n",
        "\n",
        "            X_batch = compute_x(features[start:end], vocab, max_len=max_len)\n",
        "            y_batch = np.array(labels[start:end])\n",
        "            yield X_batch, y_batch\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdlMyC7EJ6Y3",
        "colab_type": "text"
      },
      "source": [
        "utils.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meJgX0e_J8pW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# import config\n",
        "\n",
        "\n",
        "def read_txt(filename: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Read the dataset line by line.\n",
        "    :param filename: file to read\n",
        "    :return: a list of lines\n",
        "    \"\"\"\n",
        "    with open(filename, encoding=\"utf8\") as file:\n",
        "        f = (line.strip() for line in file)\n",
        "        return [line for line in f if line]\n",
        "\n",
        "\n",
        "def write_txt(filename: str, lines: List[str]):\n",
        "    \"\"\"\n",
        "    Writes a list of string in a file.\n",
        "    :param filename: path where to save the file.\n",
        "    :param lines: list of strings to serilize.\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    with open(filename, \"w\", encoding=\"utf8\") as file:\n",
        "        file.writelines(line + \"\\n\" for line in lines)\n",
        "\n",
        "\n",
        "def read_dictionary(filename: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Open a dictionary from file, in the format key -> value\n",
        "    :param filename: file to read.\n",
        "    :return: a dictionary.\n",
        "    \"\"\"\n",
        "    with open(filename) as file:\n",
        "        return {k: v for k, *v in (l.split() for l in file)}\n",
        "\n",
        "\n",
        "def write_dictionary(filename: str, dictionary: Dict):\n",
        "    \"\"\"\n",
        "    Writes a dictionary as a file.\n",
        "    :param filename: file where to save the dictionary.\n",
        "    :param dictionary: dictionary to serialize.\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    with open(filename, mode=\"w\") as file:\n",
        "        for k, v in dictionary.items():\n",
        "            file.write(k + \" \" + str(v) + \"\\n\")\n",
        "\n",
        "\n",
        "def merge_txt_files(input_file: List[str], output_filename: str):\n",
        "    \"\"\"\n",
        "    Merge the given text files.\n",
        "    :param input_file: list of strings.\n",
        "    :param output_filename: filename of the output file\n",
        "    \"\"\"\n",
        "    with open(output_filename, \"w\", encoding=\"utf8\") as out_file:\n",
        "        out_file.writelines(line + \"\\n\" for line in input_file)\n",
        "\n",
        "\n",
        "def load_datasets(crisis_path, normal_path, limit: int = 30000) -> Tuple[List[str], List[int]]:\n",
        "    \"\"\"\n",
        "    This method is used to handle all datasets path to read.\n",
        "    :return: a list of tweets\n",
        "    \"\"\"\n",
        "    # parse crisis tweets\n",
        "    crisis_tweets = read_datasets(crisis_path, limit)\n",
        "    crisis_tweets_label = [1] * len(crisis_tweets)\n",
        "    print(\"Number of crisis tweets:\", len(crisis_tweets))\n",
        "\n",
        "    # parse non-crisis tweets\n",
        "    normal_tweets = read_datasets(normal_path, limit)\n",
        "    normal_tweets_label = [0] * len(normal_tweets)\n",
        "    print(\"Number of non-crisis tweets:\", len(normal_tweets))\n",
        "    return crisis_tweets + normal_tweets, crisis_tweets_label + normal_tweets_label\n",
        "\n",
        "\n",
        "def read_datasets(path, limit: int = 30000) -> List[str]:\n",
        "    \"\"\"\n",
        "    Read crisis tweets from crisisnlp folder.\n",
        "    :return: list of tweets.\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    for file in path.glob(\"./*.txt\"):\n",
        "        tweets += read_txt(file)[:limit]\n",
        "    return tweets\n",
        "\n",
        "\n",
        "def read_crisilex() -> List[str]:\n",
        "    \"\"\"\n",
        "    Read crisis tweets from crisislex folder.\n",
        "    :return: list of tweets.\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    for file in CRISISLEX_DIR.glob(\"./*.csv\"):\n",
        "        tweets += _read_crisislex_csv(file)\n",
        "    return tweets\n",
        "\n",
        "\n",
        "def read_normal() -> List[str]:\n",
        "    \"\"\"\n",
        "    Read non-crisis tweets from normal folder.\n",
        "    :return: list of tweets.\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    for file in list(NORMAL_TRAIN_DIR.glob(\"./*.csv\"))[:13]:\n",
        "        tweets += _read_csv(file)\n",
        "    return tweets\n",
        "\n",
        "\n",
        "def _read_csv(filename) -> List[str]:\n",
        "    \"\"\"\n",
        "    Extract tweet from csv file.\n",
        "    :param filename: csv file.\n",
        "    :return: a list of tweets.\n",
        "    \"\"\"\n",
        "    with open(filename, encoding=\"latin1\") as csv_file:\n",
        "        csv_reader = csv.reader(csv_file, delimiter=\",\")\n",
        "        next(csv_reader)\n",
        "        return [row[-1] for row in csv_reader]\n",
        "\n",
        "\n",
        "def _read_crisislex_csv(filename) -> List[str]:\n",
        "    \"\"\"\n",
        "    Extract tweet from csv file.\n",
        "    :param filename: csv file.\n",
        "    :return: a list of tweets.\n",
        "    \"\"\"\n",
        "    with open(filename, encoding=\"utf8\") as csv_file:\n",
        "        csv_reader = csv.reader(csv_file, delimiter=\",\")\n",
        "        next(csv_reader)\n",
        "        return [row[1] for row in csv_reader]\n",
        "\n",
        "\n",
        "def split_csv(filename: str, n_split: int):\n",
        "    \"\"\"\n",
        "    Split a large text file in smaller files.\n",
        "    :param filename: file to split.\n",
        "    :param n_split: number of parts to split.\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    with open(filename, encoding=\"latin1\") as csv_file:\n",
        "        csv_reader = csv.reader(csv_file, delimiter=\",\")\n",
        "        next(csv_reader)\n",
        "        for row in csv_reader:\n",
        "            data.append(row[-1])\n",
        "\n",
        "    batch = len(data) // n_split\n",
        "    for i in range(0, len(data), batch):\n",
        "        j = i + batch\n",
        "        filename_batch = str(filename).split(\".\")[0] + \"_\" + str(n_split) + \".txt\"\n",
        "        print(\"Writing\", filename_batch)\n",
        "        write_txt(filename_batch, data[i:j])\n",
        "        n_split -= 1\n",
        "\n",
        "\n",
        "def split_txt(filename: str, n_split: int):\n",
        "    \"\"\"\n",
        "    Split a large text file in smaller files.\n",
        "    :param filename: file to split.\n",
        "    :param n_split: number of parts to split.\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    data = read_txt(filename)\n",
        "    batch = len(data) // n_split\n",
        "    for i in range(0, len(data), batch):\n",
        "        j = i + batch\n",
        "        filename_batch = str(filename).split(\".\")[0] + \"_\" + str(n_split) + \".txt\"\n",
        "        print(\"Writing\", filename_batch)\n",
        "        write_txt(filename_batch, data[i:j])\n",
        "        n_split -= 1\n",
        "\n",
        "\n",
        "def unzip_all(paths: list):\n",
        "    \"\"\"\n",
        "    This method is used to unzip all files.\n",
        "    :param paths: a list of path to unzip\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    for path in paths:\n",
        "        for subdirs, dirs, files in os.walk(path):\n",
        "            for file in files:\n",
        "                _, file_ext = os.path.splitext(file)\n",
        "                if file_ext == \".zip\":\n",
        "                    unzip(subdirs, file)\n",
        "\n",
        "\n",
        "def unzip(main_folder: str, file: str):\n",
        "    \"\"\"\n",
        "    This method is used to unzip a file\n",
        "    :param main_folder: folder to extract\n",
        "    :param file: file to unzip\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    zip_ref = zipfile.ZipFile(os.path.join(main_folder, file), \"r\")\n",
        "    zip_ref.extractall(main_folder)\n",
        "    zip_ref.close()\n",
        "\n",
        "\n",
        "def clear_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    This method remove from a string of text\n",
        "    every special character (tags,hash-tag, number,\n",
        "    url,etc).\n",
        "    :param text: a string of text\n",
        "    :return: a string (a text) without\n",
        "    special character\n",
        "    \"\"\"\n",
        "\n",
        "    return \" \".join((\" \".join(re.compile(\"[^a-zA-Z\\d\\s:]\").split(text))).split())\n",
        "\n",
        "\n",
        "def stop_words(all_language: bool = False) -> list:\n",
        "    \"\"\"\n",
        "    This method is used to generate stop words.\n",
        "    The default language is English but setting\n",
        "    the boolean variable to true it generates the\n",
        "    stop words for all language.\n",
        "    :param all_language: a boolean variable used\n",
        "    as flag for the languages.\n",
        "    :return: a list containing the stop\n",
        "    words.\n",
        "    \"\"\"\n",
        "    nltk.download(\"stopwords\", quiet=True)\n",
        "\n",
        "    if all_language:\n",
        "        return stopwords.words(stopwords.fileids())\n",
        "    else:\n",
        "        return stopwords.words(\"english\")\n",
        "\n",
        "\n",
        "def restrict_w2v(w2v, restricted_word_set):\n",
        "    \"\"\"\n",
        "    Retrain from w2v model only words in the restricted word set.\n",
        "    :param w2v:\n",
        "    :param restricted_word_set:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    new_vectors = []\n",
        "    new_vocab = {}\n",
        "    new_index2entity = []\n",
        "    new_vectors_norm = []\n",
        "\n",
        "    for i in range(len(w2v.vocab)):\n",
        "        word = w2v.index2entity[i]\n",
        "        vec = w2v.vectors[i]\n",
        "        vocab = w2v.vocab[word]\n",
        "        vec_norm = w2v.vectors_norm[i] if w2v.vectors_norm else []\n",
        "        if word in restricted_word_set:\n",
        "            vocab.index = len(new_index2entity)\n",
        "            new_index2entity.append(word)\n",
        "            new_vocab[word] = vocab\n",
        "            new_vectors.append(vec)\n",
        "            if vec_norm:\n",
        "                new_vectors_norm.append(vec_norm)\n",
        "\n",
        "    w2v.vocab = new_vocab\n",
        "    w2v.vectors = np.array(new_vectors)\n",
        "    w2v.index2entity = np.array(new_index2entity)\n",
        "    w2v.index2word = np.array(new_index2entity)\n",
        "    if new_vectors_norm:\n",
        "        w2v.vectors_norm = np.array(new_vectors_norm)\n",
        "    return w2v\n",
        "\n",
        "\n",
        "def clean_embeddings(path_input: str, path_output: str, size: int):\n",
        "    \"\"\"\n",
        "    Clean embeddings by removing non lemma_synset vectors.\n",
        "    :param path_input: path to original embeddings.\n",
        "    :param path_output: path to cleaned embeddings.\n",
        "    :param size:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    old_emb = read_txt(path_input)\n",
        "    filtered = [vector for vector in old_emb if \"_bn:\" in vector]\n",
        "    write_txt(path_output, [str(len(filtered)) + \" \" + str(size)] + filtered)\n",
        "\n",
        "\n",
        "def timer(start: float, end: float) -> str:\n",
        "    \"\"\"\n",
        "    Timer function. Compute execution time from strart to end (end - start).\n",
        "    :param start: start time\n",
        "    :param end: end time\n",
        "    :return: end - start\n",
        "    \"\"\"\n",
        "    hours, rem = divmod(end - start, 3600)\n",
        "    minutes, seconds = divmod(rem, 60)\n",
        "    return \"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours), int(minutes), seconds)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNcSAoQ9J9WV",
        "colab_type": "text"
      },
      "source": [
        "evaluation.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJCSrM-QKAcy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "5966f5cf-7ce3-4b7b-bf9f-4cd3fff78a77"
      },
      "source": [
        "import gensim\n",
        "import sklearn\n",
        "from tqdm import tqdm\n",
        "\n",
        "# import utils\n",
        "# import preprocess\n",
        "# import config\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def classification_report(y_test, y_pred, target):\n",
        "    \"\"\"\n",
        "    Compute a classification report using sklearn.\n",
        "    :param y_test: true labels.\n",
        "    :param y_pred: predicted labels.\n",
        "    :param target: label names.\n",
        "    :return: a classification report from sklearn.\n",
        "    \"\"\"\n",
        "    return sklearn.metrics.classification_report(\n",
        "        y_test, y_pred, target_names=target\n",
        "    )\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    \"\"\"\n",
        "    Evaluate the model printing a classification report (acc, prec, recall and f1).\n",
        "    :param model: the trained model.\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    x_test, y_test = load_datasets(CRISIS_EVAL_DIR, NORMAL_EVAL_DIR, limit=1000)\n",
        "    x_test = clear_tweets(x_test)\n",
        "    vocab = read_dictionary(TRAIN_VOCAB)\n",
        "    x_test = compute_x(x_test, vocab, max_len=100)[:, :, 0]\n",
        "    y_pred = model.predict(x_test, batch_size=64)\n",
        "    y_pred = [1 if y > 0.5 else 0 for y in y_pred]\n",
        "    cr = classification_report(y_test, y_pred, [\"normal\", \"crisis\"])\n",
        "    print(\"Classification report : \\n\", cr)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = tf.keras.models.load_model(str(OUTPUT_DIR / \"model.h5\"))\n",
        "    evaluate(model)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-e559cd9ae66a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    144\u001b[0m       h5py is not None and (\n\u001b[1;32m    145\u001b[0m           isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_from_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    198\u001b[0m   \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'resources/output/model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe0eTaFyKEIn",
        "colab_type": "text"
      },
      "source": [
        "train.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqwXPJ_XKFVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# import config\n",
        "# import models\n",
        "# import utils\n",
        "# from sequence import TextSequence\n",
        "\n",
        "\n",
        "def train_keras(\n",
        "    features,\n",
        "    labels,\n",
        "    w2v,\n",
        "    w2v_vocab,\n",
        "    epochs: int = 5,\n",
        "    hidden_size: int = 100,\n",
        "    batch_size: int = 2048,\n",
        "):\n",
        "\n",
        "    tweets_tr, tweets_dev, labels_tr, labels_dev = train_test_split(\n",
        "        features, labels, test_size=0.10\n",
        "    )\n",
        "\n",
        "    train_gen = TextSequence(\n",
        "        tweets_tr, labels_tr, vocab=w2v_vocab, batch_size=batch_size, max_len=100\n",
        "    )\n",
        "\n",
        "    dev_gen = TextSequence(\n",
        "        tweets_dev, labels_dev, vocab=w2v_vocab, batch_size=batch_size, max_len=100\n",
        "    )\n",
        "\n",
        "    model = build_model(\n",
        "        layer=tf.keras.layers.GRU,\n",
        "        hidden_size=hidden_size,\n",
        "        dropout=0.4,\n",
        "        recurrent_dropout=0.2,\n",
        "        vocab_size=len(w2v_vocab),\n",
        "        word2vec=w2v,\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    start = time.time()\n",
        "    history = model.fit_generator(\n",
        "        train_gen,\n",
        "        steps_per_epoch=len(tweets_tr) // batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=dev_gen,\n",
        "        validation_steps=len(tweets_dev) // batch_size,\n",
        "        shuffle=True\n",
        "        # callbacks=[es, cp],\n",
        "    )\n",
        "    end = time.time()\n",
        "    print(timer(start, end))\n",
        "    model.save(str(OUTPUT_DIR / \"model.h5\"))\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3Itq7SpJi_R",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}